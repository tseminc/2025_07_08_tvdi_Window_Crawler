import os
import asyncio
from crawl4ai import AsyncWebCrawler,CrawlerRunConfig,CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

if os.name == 'nt':
    os.system('cls')
else:
    os.system('clear')  

def process_datas(datas : list):
    for item in datas:
        print(item)    

async def main():
    url = 'https://rate.bot.com.tw/xrt?Lang=zh-TW'

    schema = {
        "name":"台幣匯率",
        "baseSelector":"table[title='牌告匯率'] tr",
        "fields":[
            {
                "name":"幣名",
                "selector":"td[data-table='幣別'] div.visible-phone.print_hide",
                "type":"text"
            },
            {
                "name":"現金匯率_本行買入",
                "selector":'[data-table="本行現金買入"]',
                "type":"text"
            },
            {
                "name":"現金匯率_本行賣出",
                "selector":'[data-table="本行現金賣出"]',
                "type":"text"
            },
            {
                "name":"即期匯率_本行買入",
                "selector":'[data-table="本行即期買入"]',
                "type":"text"
            },
            {
                "name":"即期匯率_本行賣出",
                "selector":'[data-table="本行即期賣出"]',
                "type":"text"
            }
        ]
    }

    #CrawlerRunConfig實體
    run_config = CrawlerRunConfig(
        cache_mode = CacheMode.BYPASS,
        extraction_strategy=JsonCssExtractionStrategy(schema=schema)
    )

    #建立一個AsyncWebCrawler的實體
    async with AsyncWebCrawler() as crawler:
        #Run the crawler on a URL
        result = await crawler.arun(
            url=url,
            config=run_config
        )
        # print(type(result.extracted_content)) 
        # print(result.extracted_content)
        datas : list = json.loads(result.extracted_content)
        process_datas(datas)
        

if __name__ == "__main__":
    asyncio.run(main())